{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2bdf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the files\n",
    "file_path_animal = '/content/Animal_movie.csv'\n",
    "file_path_sambahadur = '/content/Sambahadur_movie.csv'\n",
    "\n",
    "# Reading the CSV files\n",
    "animal_data = pd.read_csv(file_path_animal)\n",
    "sambahadur_data = pd.read_csv(file_path_sambahadur)\n",
    "\n",
    "# Displaying the first few rows of each file to understand their structure\n",
    "animal_data_head = animal_data.head()\n",
    "sambahadur_data_head = sambahadur_data.head()\n",
    "\n",
    "animal_data_head[\"Tweets\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a931954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "\n",
    "def analyze_sentiment(df):\n",
    "    \"\"\" Function to analyze the sentiment of tweets in a dataframe \"\"\"\n",
    "    # Applying TextBlob sentiment analysis\n",
    "    df['Polarity'] = df['Tweets'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    df['Sentiment'] = np.select(\n",
    "        [\n",
    "            df['Polarity'] > 0,\n",
    "            df['Polarity'] == 0,\n",
    "            df['Polarity'] < 0\n",
    "        ],\n",
    "        [\n",
    "            'Positive',\n",
    "            'Neutral',\n",
    "            'Negative'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Summary of sentiments\n",
    "    sentiment_summary = df['Sentiment'].value_counts(normalize=True) * 100\n",
    "    return sentiment_summary\n",
    "\n",
    "# Analyze sentiment for both datasets\n",
    "sentiment_summary_animal = analyze_sentiment(animal_data)\n",
    "sentiment_summary_sambahadur = analyze_sentiment(sambahadur_data)\n",
    "\n",
    "sentiment_summary_animal, sentiment_summary_sambahadur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f95872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import itertools\n",
    "\n",
    "def extract_keywords(df):\n",
    "    \"\"\" Function to extract keywords from tweets \"\"\"\n",
    "    # Using CountVectorizer to extract keywords\n",
    "    vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,2), max_features=10)\n",
    "    counts = vectorizer.fit_transform(df['Tweets'])\n",
    "    keywords = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Count the occurrences of each keyword\n",
    "    counts_sum = counts.sum(axis=0)\n",
    "    keywords_freq = [(word, counts_sum[0, idx]) for word, idx in zip(keywords, range(counts_sum.shape[1]))]\n",
    "\n",
    "    # Sort keywords based on frequency\n",
    "    sorted_keywords = sorted(keywords_freq, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_keywords\n",
    "\n",
    "# Extract keywords for both datasets\n",
    "keywords_animal = extract_keywords(animal_data)\n",
    "keywords_sambahadur = extract_keywords(sambahadur_data)\n",
    "\n",
    "keywords_animal, keywords_sambahadur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745169cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def perform_topic_modeling(df):\n",
    "    \"\"\" Function to perform topic modeling on tweets \"\"\"\n",
    "    # Using TfidfVectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tfidf = tfidf_vectorizer.fit_transform(df['Tweets'])\n",
    "\n",
    "    # Using LDA for topic modeling\n",
    "    lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "    lda.fit(tfidf)\n",
    "\n",
    "    # Extracting the topics\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        topic_keywords = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
    "        topics.append((f\"Topic {topic_idx+1}\", topic_keywords))\n",
    "\n",
    "    return topics\n",
    "\n",
    "# Perform topic modeling for both datasets\n",
    "topics_animal = perform_topic_modeling(animal_data)\n",
    "topics_sambahadur = perform_topic_modeling(sambahadur_data)\n",
    "\n",
    "topics_animal, topics_sambahadur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def summarize_text(df, n_components=1):\n",
    "    \"\"\" Summarize text using LSA (Latent Semantic Analysis) \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "    X = vectorizer.fit_transform(df['Tweets'])\n",
    "\n",
    "    # Using TruncatedSVD for LSA\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    X_reduced = svd.fit_transform(X)\n",
    "\n",
    "    # Extracting the components\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    summary = []\n",
    "    for i, comp in enumerate(svd.components_):\n",
    "        terms_comp = zip(terms, comp)\n",
    "        sorted_terms = sorted(terms_comp, key=lambda x: x[1], reverse=True)[:10]\n",
    "        summary.append(\" \".join([t[0] for t in sorted_terms]))\n",
    "\n",
    "    return ' '.join(summary)\n",
    "\n",
    "# Generating summaries\n",
    "summary_animal = summarize_text(animal_data)\n",
    "summary_sambahadur = summarize_text(sambahadur_data)\n",
    "\n",
    "summary_animal, summary_sambahadur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b924f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming we have two datasets, we'll concatenate them for this example\n",
    "combined_tweets = pd.concat([animal_data['Tweets'], sambahadur_data['Tweets']])\n",
    "\n",
    "# Combine the tweets from both DataFrames\n",
    "texts = pd.concat([animal_data['Tweets'], sambahadur_data['Tweets']]).tolist()\n",
    "\n",
    "# Create dummy labels for example purposes\n",
    "# In a real application, these should be actual sentiment labels\n",
    "labels = [1 if i < len(texts) / 2 else 0 for i in range(len(texts))]\n",
    "\n",
    "\n",
    "# Tokenizing the tweets\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(combined_tweets)\n",
    "sequences = tokenizer.texts_to_sequences(combined_tweets)\n",
    "\n",
    "# Padding the sequences to ensure uniform length\n",
    "max_sequence_length = 200\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
    "    padded_sequences, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "train_sequences.shape, test_sequences.shape, len(train_labels), len(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b80acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample preprocessing steps\n",
    "# Assuming 'texts' is your list of tweets and 'labels' are the corresponding sentiments\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200)\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "# Split the data into training and testing sets\n",
    "# train_sequences, test_sequences, train_labels, test_labels\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_sequences, train_labels, batch_size=32, epochs=10, validation_data=(test_sequences, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "# model.evaluate(test_sequences, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e86e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming sentiment_summary_animal and sentiment_summary_sambahadur contain the sentiment distribution\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "df_animal_sentiments = sentiment_summary_animal.reset_index()\n",
    "df_animal_sentiments.columns = ['Sentiment', 'Percentage']\n",
    "\n",
    "df_sambahadur_sentiments = sentiment_summary_sambahadur.reset_index()\n",
    "df_sambahadur_sentiments.columns = ['Sentiment', 'Percentage']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='Sentiment', y='Percentage', data=df_animal_sentiments)\n",
    "plt.title('Sentiment Distribution for Animal Movie')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='Sentiment', y='Percentage', data=df_sambahadur_sentiments)\n",
    "plt.title('Sentiment Distribution for Sambahadur Movie')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'history' is the object returned by the fit method of your model\n",
    "# Replace 'history' with your actual history object\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# import numpy as np\n",
    "\n",
    "# # Sample Data Preparation\n",
    "# texts = pd.concat([animal_data['Tweets'], sambahadur_data['Tweets']]).tolist()\n",
    "# labels = ['Positive', 'Neutral', 'Negative']\n",
    "\n",
    "# # Assuming you have your text data in 'texts' and labels in 'labels'\n",
    "# # Tokenize the texts\n",
    "# tokenizer = Tokenizer(num_words=5000)\n",
    "# tokenizer.fit_on_texts(texts)\n",
    "# sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# # Padding sequences to ensure uniform length\n",
    "# max_sequence_length = 200\n",
    "# padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# # Convert labels to categorical (one-hot encoding)\n",
    "# label_mapping = {'Positive': 0, 'Neutral': 1, 'Negative': 2}\n",
    "# labels_numerical = [label_mapping[label] for label in labels]\n",
    "# labels_categorical = to_categorical(labels_numerical, num_classes=3)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
    "#     padded_sequences, labels_categorical, test_size=0.25, random_state=42)\n",
    "\n",
    "# # LSTM Model\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=5000, output_dim=128, input_length=max_sequence_length))\n",
    "# model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(3, activation='softmax')) # 3 neurons for 3 classes\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(train_sequences, train_labels, batch_size=32, epochs=10, validation_data=(test_sequences, test_labels))\n",
    "\n",
    "# # Evaluate the model\n",
    "# # model.evaluate(test_sequences, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a80e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "texts = pd.concat([animal_data['Tweets'], sambahadur_data['Tweets']]).tolist()\n",
    "labels = ['Positive', 'Neutral', 'Negative'] * (len(texts) // 3)\n",
    "\n",
    "if len(labels) < len(texts):\n",
    "    labels += ['Positive'] * (len(texts) - len(labels))\n",
    "# Ensure the number of texts matches the number of labels\n",
    "assert len(texts) == len(labels), \"The number of texts and labels must be the same.\"\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Padding sequences to ensure uniform length\n",
    "max_sequence_length = 200\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "label_mapping = {'Positive': 0, 'Neutral': 1, 'Negative': 2}\n",
    "labels_numerical = [label_mapping[label] for label in labels]\n",
    "labels_categorical = to_categorical(labels_numerical, num_classes=3)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
    "    padded_sequences, labels_categorical, test_size=0.25, random_state=42, stratify=labels_categorical)\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=512, input_length=max_sequence_length))\n",
    "model.add(LSTM(512, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))  # 3 neurons for 3 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_sequences, train_labels, batch_size=32, epochs=10, validation_data=(test_sequences, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "# model.evaluate(test_sequences, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62d9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts = pd.concat([animal_data['Tweets'], sambahadur_data['Tweets']]).tolist()\n",
    "labels = ['Positive', 'Neutral', 'Negative'] * (len(texts) // 3)\n",
    "\n",
    "if len(labels) < len(texts):\n",
    "    labels += ['Positive'] * (len(texts) - len(labels))\n",
    "# Ensure the number of texts matches the number of labels\n",
    "assert len(texts) == len(labels), \"The number of texts and labels must be the same.\"\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "label_mapping = {'Positive': 0, 'Neutral': 1, 'Negative': 2}\n",
    "labels_numerical = [label_mapping[label] for label in labels]\n",
    "labels_categorical = to_categorical(labels_numerical, num_classes=3)\n",
    "\n",
    "# Split the data\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
    "    padded_sequences, labels_categorical, test_size=0.25, random_state=42)\n",
    "\n",
    "# Building the LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_sequences, train_labels, batch_size=32, epochs=10, validation_data=(test_sequences, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.evaluate(test_sequences, test_labels)[1]\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8475ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts = pd.concat([animal_data['Tweets'], sambahadur_data['Tweets']]).tolist()\n",
    "labels = ['Positive', 'Neutral', 'Negative'] * (len(texts) // 3)\n",
    "\n",
    "if len(labels) < len(texts):\n",
    "    labels += ['Positive'] * (len(texts) - len(labels))\n",
    "# Ensure the number of texts matches the number of labels\n",
    "assert len(texts) == len(labels), \"The number of texts and labels must be the same.\"\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "label_mapping = {'Positive': 0, 'Neutral': 1, 'Negative': 2}\n",
    "labels_numerical = [label_mapping[label] for label in labels]\n",
    "labels_categorical = to_categorical(labels_numerical, num_classes=3)\n",
    "\n",
    "# Split the data\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
    "    padded_sequences, labels_categorical, test_size=0.25, random_state=42)\n",
    "\n",
    "# Building the LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_sequences, train_labels, batch_size=32, epochs=10, validation_data=(test_sequences, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.evaluate(test_sequences, test_labels)[1]\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e55455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts = animal_data['Tweets'].tolist()\n",
    "labels = ['Positive', 'Neutral', 'Negative'] * (len(texts) // 3)\n",
    "\n",
    "if len(labels) < len(texts):\n",
    "    labels += ['Positive'] * (len(texts) - len(labels))\n",
    "# Ensure the number of texts matches the number of labels\n",
    "assert len(texts) == len(labels), \"The number of texts and labels must be the same.\"\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "label_mapping = {'Positive': 0, 'Neutral': 1, 'Negative': 2}\n",
    "labels_numerical = [label_mapping[label] for label in labels]\n",
    "labels_categorical = to_categorical(labels_numerical, num_classes=3)\n",
    "\n",
    "# Split the data\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
    "    padded_sequences, labels_categorical, test_size=0.25, random_state=42)\n",
    "\n",
    "# Building the LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_sequences, train_labels, batch_size=32, epochs=10, validation_data=(test_sequences, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.evaluate(test_sequences, test_labels)[1]\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb26d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "# Assuming data preprocessing is done\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_sequences, train_labels, batch_size=32, epochs=15, validation_data=(test_sequences, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdffb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'history' is the return value from model.fit()\n",
    "\n",
    "# Plot for accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944de688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tweets = animal_data['Tweets'].tolist() + sambahadur_data['Tweets'].tolist()\n",
    "# Concatenate all tweets into a single string\n",
    "combined_text = ' '.join(tweets)  # Replace 'tweets' with your list of tweets\n",
    "\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(background_color='white', max_words=100, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate and show the word cloud\n",
    "wordcloud.generate(combined_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
